{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cd0aa3",
   "metadata": {},
   "source": [
    "## üß© Document Object + Metadata Demo (LangChain)\n",
    "\n",
    "This script demonstrates how LangChain represents data using the `Document` class\n",
    "and why **metadata** is important in RAG and retrieval systems.\n",
    "\n",
    "### ‚úîÔ∏è What this script does\n",
    "\n",
    "- Imports LangChain text utilities & `Document`\n",
    "- Creates a sample `Document` containing:\n",
    "  - `page_content` ‚Üí the actual text\n",
    "  - `metadata` ‚Üí contextual information about the text\n",
    "- Prints both content and metadata to show the structure\n",
    "- Explains why metadata is useful\n",
    "\n",
    "### ‚öôÔ∏è Used uv in place of pip\n",
    "\n",
    "This project uses uv instead of pip because it is faster and automatically manages virtual environments.\n",
    "\n",
    "‚úîÔ∏è Commands Used\n",
    "- Initialize the project:\n",
    "- uv init\n",
    "- Install dependencies from requirements.txt:\n",
    "- uv add -r requirements.txt\n",
    "\n",
    "Add an additional package:\n",
    "- uv add pandas\n",
    "\n",
    "Run the script using uv:\n",
    "- uv run python script.py\n",
    "\n",
    "### üí° Why uv?\n",
    "\n",
    "- Faster than pip\n",
    "- Creates and manages virtual environments automatically\n",
    "- Keeps dependencies isolated and reproducible\n",
    "- Well-suited for modern Python + RAG projects\n",
    "\n",
    "### üóÇÔ∏è What is a `Document`?\n",
    "\n",
    "A `Document` in LangChain stores:\n",
    "\n",
    "- **Text** (`page_content`)\n",
    "- **Metadata** (`metadata = {...}`)\n",
    "\n",
    "Example metadata fields:\n",
    "- source / filename  \n",
    "- page number  \n",
    "- author  \n",
    "- timestamps  \n",
    "- tags or category  \n",
    "\n",
    "Metadata helps during:\n",
    "- üîç Search & filtering\n",
    "- üß† Context-aware retrieval\n",
    "- üìö Grouping and organization\n",
    "- üßæ Auditing and traceability\n",
    "\n",
    "### ‚úÇÔ∏è Why we imported text splitters?\n",
    "\n",
    "The splitters are used later to:\n",
    "1. Break large documents into smaller chunks\n",
    "2. Feed chunks into embeddings / vector DBs\n",
    "3. Build RAG pipelines efficiently\n",
    "\n",
    "(They are imported here for later use in the workflow.)\n",
    "\n",
    "### üéØ Key takeaway\n",
    "\n",
    "This script is about **understanding the structure of a Document object**\n",
    "before moving into:\n",
    "- text splitting\n",
    "- embeddings\n",
    "- vector storage\n",
    "- RAG pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655aafd",
   "metadata": {},
   "source": [
    "# Introduction to dataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32b19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List,Dict,Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a8ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed\\OneDrive\\Desktop\\ExploringRAGs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up Completed\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    \n",
    ")\n",
    "print(\"Set up Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6934e",
   "metadata": {},
   "source": [
    "### Understanding Document structure in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19bb41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content :This is a sample document.\n",
      "Metadata :{'source': 'sample.txt', 'page': 1, 'author': 'Ahmed', 'date_created': '2026-01-03'}\n",
      "Metadata is crucial for:\n",
      "- Contextual Understanding\n",
      "- Efficient Retrieval\n",
      "- Enhanced Searchability\n",
      "- Data Management\n",
      "- Compliance and Auditing\n"
     ]
    }
   ],
   "source": [
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content = \"This is a sample document.\",\n",
    "    metadata = {\n",
    "        \"source\":\"sample.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"Ahmed\",\n",
    "        \"date_created\":\"2026-01-03\"\n",
    "    }\n",
    ")\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(f\"Content :{doc.page_content}\")\n",
    "print(f\"Metadata :{doc.metadata}\")\n",
    "\n",
    "# Why metadata matters:\n",
    "print(\"Metadata is crucial for:\")\n",
    "print(\"- Contextual Understanding\")\n",
    "print(\"- Efficient Retrieval\")\n",
    "print(\"- Enhanced Searchability\")\n",
    "print(\"- Data Management\")\n",
    "print(\"- Compliance and Auditing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5fbdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d76e0",
   "metadata": {},
   "source": [
    "## üìÇ Text File Loading Demo ‚Äî TextLoader & DirectoryLoader (LangChain)\n",
    "\n",
    "This script demonstrates how to create sample text files and load them into\n",
    "LangChain `Document` objects for use in RAG or NLP pipelines.\n",
    "\n",
    "### ‚úîÔ∏è What the script does\n",
    "\n",
    "1. Creates a `data/text_files/` folder\n",
    "2. Saves three sample `.txt` files into it\n",
    "3. Loads a **single file** using `TextLoader`\n",
    "4. Loads **all files in the folder** using `DirectoryLoader`\n",
    "5. Prints document content previews and metadata\n",
    "6. Displays pros & cons of using `DirectoryLoader`\n",
    "\n",
    "### üßæ Why this is useful\n",
    "\n",
    "- Converts raw text files into structured `Document` objects  \n",
    "- Preserves metadata like file path (important for search & traceability)  \n",
    "- Prepares text for:\n",
    "  - splitting\n",
    "  - embeddings\n",
    "  - vector databases\n",
    "  - RAG pipelines\n",
    "\n",
    "### ‚öôÔ∏è Tools Used\n",
    "\n",
    "- **TextLoader** ‚Üí loads one file at a time  \n",
    "- **DirectoryLoader** ‚Üí loads multiple files in bulk using glob patterns  \n",
    "\n",
    "### üí° Key takeaway\n",
    "\n",
    "This script is a foundation step before:\n",
    "- chunking text\n",
    "- embedding documents\n",
    "- building retrieval-augmented applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3eb8b2",
   "metadata": {},
   "source": [
    "## Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57641f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7bc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text added succesfully to files!\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"data/text_files/python_intro.txt\": \"\"\"\n",
    "Python is a high-level, interpreted programming language\n",
    "known for its readability and versatility. It is widely\n",
    "used in web development, data science, automation,\n",
    "machine learning, and scripting.\n",
    "\n",
    "Python emphasizes code simplicity and developer productivity.\n",
    "Its large ecosystem of libraries makes it one of the most\n",
    "popular programming languages in the world.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/ai_overview.txt\": \"\"\"\n",
    "Artificial Intelligence (AI) refers to systems designed\n",
    "to perform tasks that normally require human intelligence,\n",
    "such as reasoning, learning, perception, and language\n",
    "understanding.\n",
    "\n",
    "Modern AI applications include chatbots, recommendation\n",
    "systems, autonomous vehicles, and predictive analytics.\n",
    "\"\"\",\n",
    "\n",
    "    \"data/text_files/cloud_computing.txt\": \"\"\"\n",
    "Cloud computing allows users to store and process data\n",
    "over the internet instead of on local machines.\n",
    "\n",
    "It offers benefits such as scalability, flexibility,\n",
    "cost efficiency, and remote accessibility.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "print(\"Sample text added succesfully to files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9def436",
   "metadata": {},
   "source": [
    "## TextLoader - Read Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712ef648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='\\nPython is a high-level, interpreted programming language\\nknown for its readability and versatility. It is widely\\nused in web development, data science, automation,\\nmachine learning, and scripting.\\n\\nPython emphasizes code simplicity and developer productivity.\\nIts large ecosystem of libraries makes it one of the most\\npopular programming languages in the world.\\n')]\n",
      "Loaded 1 document\n",
      "Content preview: \n",
      "Python is a high-level, interpreted programming language\n",
      "known for its readability and versatility....\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "## Loading a single text file\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "\n",
    "document = loader.load()\n",
    "# print(type(document))\n",
    "print(document)\n",
    "\n",
    "print(f\"Loaded {len(document)} document\")\n",
    "print(f\"Content preview: {document[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {document[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888e6c5",
   "metadata": {},
   "source": [
    "## DirectoryLoader-Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aaa8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 136.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 3 documents from directory.\n",
      "\n",
      "Document 1:\n",
      "Content preview: \n",
      "Artificial Intelligence (AI) refers to systems designed\n",
      "to perform tasks that normally require huma...\n",
      "Source: data\\text_files\\ai_overview.txt\n",
      "Length: 298 characters\n",
      "\n",
      "Document 2:\n",
      "Content preview: \n",
      "Cloud computing allows users to store and process data\n",
      "over the internet instead of on local machin...\n",
      "Source: data\\text_files\\cloud_computing.txt\n",
      "Length: 201 characters\n",
      "\n",
      "Document 3:\n",
      "Content preview: \n",
      "Python is a high-level, interpreted programming language\n",
      "known for its readability and versatility....\n",
      "Source: data\\text_files\\python_intro.txt\n",
      "Length: 363 characters\n",
      "\n",
      "Advantages of DirectoryLoader:\n",
      "1. Automatically loads multiple files from a directory\n",
      "2. Supports glob patterns for flexible file selection\n",
      "3. Preserves file metadata such as path and filename\n",
      "4. Useful for batch data ingestion in RAG systems\n",
      "5. Easy to combine with text splitters and embeddings\n",
      "\n",
      "Disadvantages of DirectoryLoader:\n",
      "1. Loads all files into memory ‚Äî not ideal for huge datasets\n",
      "2. Limited control over deeply nested directory structures\n",
      "3. Requires compatible loaders for each file type\n",
      "4. No built-in filtering for duplicates or noisy content\n",
      "5. Can be slower for very large folders compared to streaming loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load all the files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents from directory.\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content preview: {doc.page_content[:100]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Length: {len(doc.page_content)} characters\")\n",
    "\n",
    "\n",
    "print(\"\\nAdvantages of DirectoryLoader:\")\n",
    "print(\"1. Automatically loads multiple files from a directory\")\n",
    "print(\"2. Supports glob patterns for flexible file selection\")\n",
    "print(\"3. Preserves file metadata such as path and filename\")\n",
    "print(\"4. Useful for batch data ingestion in RAG systems\")\n",
    "print(\"5. Easy to combine with text splitters and embeddings\")\n",
    "\n",
    "print(\"\\nDisadvantages of DirectoryLoader:\")\n",
    "print(\"1. Loads all files into memory ‚Äî not ideal for huge datasets\")\n",
    "print(\"2. Limited control over deeply nested directory structures\")\n",
    "print(\"3. Requires compatible loaders for each file type\")\n",
    "print(\"4. No built-in filtering for duplicates or noisy content\")\n",
    "print(\"5. Can be slower for very large folders compared to streaming loaders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115eed40",
   "metadata": {},
   "source": [
    "## üß© Text Splitting Techniques in LangChain\n",
    "\n",
    "This guide explains three major text splitting methods used in RAG pipelines and\n",
    "document processing:\n",
    "\n",
    "- CharacterTextSplitter  \n",
    "- RecursiveCharacterTextSplitter  \n",
    "- TokenTextSplitter  \n",
    "\n",
    "It also includes an example of **overlap behavior** and a comparison summary.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Method-1: CharacterTextSplitter\n",
    "\n",
    "**How it works**\n",
    "\n",
    "- Splits text based on **fixed character length**\n",
    "- Uses a separator (e.g., space or newline)\n",
    "- Adds overlap between chunks if needed\n",
    "\n",
    "**Pros**\n",
    "- Simple and predictable\n",
    "- Uniform chunk sizes\n",
    "\n",
    "**Cons**\n",
    "- May break sentences and meaning\n",
    "- No semantic awareness\n",
    "\n",
    "**Use when**\n",
    "- Doing basic experiments\n",
    "- Chunk size consistency matters\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Method-2: RecursiveCharacterTextSplitter\n",
    "\n",
    "**How it works**\n",
    "\n",
    "- Splits text using a **hierarchical fallback strategy**  \n",
    "  paragraph ‚Üí sentence ‚Üí word ‚Üí character\n",
    "- Adds overlap **only when a chunk is actually split**\n",
    "- Produces context-preserving chunks\n",
    "\n",
    "**Pros**\n",
    "- Meaning-aware splitting\n",
    "- Works best for real documents (PDFs, articles, pages)\n",
    "- Reduces broken sentences\n",
    "\n",
    "**Cons**\n",
    "- Less predictable chunk boundaries than character splitting\n",
    "\n",
    "**Use when**\n",
    "- Building RAG systems\n",
    "- Processing long paragraphs or natural text\n",
    "\n",
    "### üîé Overlap Example\n",
    "\n",
    "- Overlap ensures the **end of one chunk is repeated in the next**\n",
    "- Prevents loss of meaning across chunk boundaries\n",
    "- Useful for embeddings & retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Method-3: TokenTextSplitter\n",
    "\n",
    "**How it works**\n",
    "\n",
    "- Splits text based on **tokens (LLM units)** rather than characters\n",
    "- Aligns with model token limits\n",
    "\n",
    "**Pros**\n",
    "- Token-safe chunks for LLMs\n",
    "- More consistent for embeddings\n",
    "- Ideal for OpenAI / transformer models\n",
    "\n",
    "**Cons**\n",
    "- Token count ‚â† character count (less human-visible)\n",
    "\n",
    "**Use when**\n",
    "- Preparing text for embeddings\n",
    "- Working with API token limits\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Quick Comparison\n",
    "\n",
    "| Splitter | Strength | Weakness | Best Use |\n",
    "|--------|--------|--------|--------|\n",
    "| Character | Simple & predictable | Breaks sentences | Basic processing |\n",
    "| Recursive | Meaning-aware | Less uniform | RAG / documents |\n",
    "| Token | Token-aligned | Harder to read | LLM & embeddings |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Choose Which?\n",
    "\n",
    "- ‚úî **RecursiveCharacterTextSplitter** ‚Üí Real-world RAG pipelines  \n",
    "- ‚úî **TokenTextSplitter** ‚Üí Embeddings & token-safe chunks  \n",
    "- ‚úî **CharacterTextSplitter** ‚Üí Simple or controlled experiments  \n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaway\n",
    "\n",
    "Different splitters exist because **different tasks need different chunk behavior**.  \n",
    "Choose the one that best preserves meaning while fitting model limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08203d74",
   "metadata": {},
   "source": [
    "## TextSplitting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0959cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='\\nPython is a high-level, interpreted programming language\\nknown for its readability and versatility. It is widely\\nused in web development, data science, automation,\\nmachine learning, and scripting.\\n\\nPython emphasizes code simplicity and developer productivity.\\nIts large ecosystem of libraries makes it one of the most\\npopular programming languages in the world.\\n')]\n"
     ]
    }
   ],
   "source": [
    "## Different text splitting techniques\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    \n",
    ")\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749fdecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character Text Splitter produced 2 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Python is a high-level, interpreted programming language\n",
      "known for its readability and versatility. It is widely\n",
      "used in web development, data science, automation,\n",
      "machine learning, and\n",
      "\n",
      "Chunk 2:\n",
      "learning, and scripting.\n",
      "\n",
      "Python emphasizes code simplicity and developer productivity.\n",
      "Its large ecosystem of libraries makes it one of the most\n",
      "popular programming languages in the world.\n"
     ]
    }
   ],
   "source": [
    "## Method-1 : Character Text Spltter\n",
    "text = document[0].page_content\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \", # Split on newlines\n",
    "    chunk_size=200, # Each chunk has 200 characters\n",
    "    chunk_overlap=20, # Overlap between chunks\n",
    "    length_function=len # Use length of text\n",
    ")\n",
    "\n",
    "\n",
    "char_chunks = char_splitter.split_text(text)\n",
    "print(f\"\\nCharacter Text Splitter produced {len(char_chunks)} chunks:\")\n",
    "for i, chunk in enumerate(char_chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f5d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recursive Character Text Splitter produced 2 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Python is a high-level, interpreted programming language\n",
      "known for its readability and versatility. It is widely\n",
      "used in web development, data science, automation,\n",
      "machine learning, and scripting.\n",
      "\n",
      "Chunk 2:\n",
      "Python emphasizes code simplicity and developer productivity.\n",
      "Its large ecosystem of libraries makes it one of the most\n",
      "popular programming languages in the world.\n"
     ]
    }
   ],
   "source": [
    "## Mathod-2: RecursiveCharacterTextSplitter\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], # Hierarchical separators\n",
    "    chunk_size=200, # Each chunk has 200 characters\n",
    "    chunk_overlap=20, # Overlap between chunks\n",
    "    length_function=len # Use length of text\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"\\nRecursive Character Text Splitter produced {len(recursive_chunks)} chunks:\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acbc1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks generated: 4\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Retrieval Augmented Generation allows language models to retrieve external\n",
      "\n",
      "--- Chunk 2 ---\n",
      "retrieve external knowledge while answering questions. It improves factual\n",
      "\n",
      "--- Chunk 3 ---\n",
      "It improves factual accuracy, reduces hallucinations, and enables real-world\n",
      "\n",
      "--- Chunk 4 ---\n",
      "enables real-world enterprise applications.\n",
      "\n",
      "üü° Overlap Check\n",
      "\n",
      "Between Chunk 1 ‚Üí 2\n",
      "Overlap: to retrieve external\n",
      "Chunk 1 ends with ... to retrieve external\n",
      "Chunk 2 starts with retrieve external kn ...\n",
      "\n",
      "Between Chunk 2 ‚Üí 3\n",
      "Overlap:  It improves factual\n",
      "Chunk 2 ends with ...  It improves factual\n",
      "Chunk 3 starts with It improves factual  ...\n",
      "\n",
      "Between Chunk 3 ‚Üí 4\n",
      "Overlap: d enables real-world\n",
      "Chunk 3 ends with ... d enables real-world\n",
      "Chunk 4 starts with enables real-world e ...\n"
     ]
    }
   ],
   "source": [
    "# Overlap example\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "Retrieval Augmented Generation allows language models to retrieve external knowledge while answering questions. It improves factual accuracy, reduces hallucinations, and enables real-world enterprise applications.\n",
    "\"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nChunks generated: {len(chunks)}\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{c}\")\n",
    "\n",
    "print(\"\\nüü° Overlap Check\")\n",
    "for i in range(len(chunks)-1):\n",
    "    overlap = chunks[i][-20:]\n",
    "    print(f\"\\nBetween Chunk {i+1} ‚Üí {i+2}\")\n",
    "    print(\"Overlap:\", overlap)\n",
    "    print(\"Chunk\", i+1, \"ends with ...\", overlap)\n",
    "    print(\"Chunk\", i+2, \"starts with\", chunks[i+1][:20], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57680759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 chunks using TokenTextSplitter\n",
      "\n",
      "Chunk 1:\n",
      " Retrieval Augmented Generation allows language models to retrieve external knowledge while answering questions. It improves factual accuracy, reduces hallucinations, and enables real-world enterprise applications. \n"
     ]
    }
   ],
   "source": [
    "## Method-3 : Token Text Splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=40,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "token_chumks = token_splitter.split_text(text)\n",
    "print(f\"Created {len(token_chumks)} chunks using TokenTextSplitter\")\n",
    "for i, chunk in enumerate(token_chumks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee15915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Comparison of Text Splitting Methods\n",
      "\n",
      "üîπ RecursiveCharacterTextSplitter\n",
      "- Smart splitting using hierarchical rules (paragraph ‚Üí sentence ‚Üí word ‚Üí char)\n",
      "- Preserves semantic meaning better\n",
      "- Adds overlap only when chunks are actually split\n",
      "- Best for RAG, documents, PDFs, articles\n",
      "\n",
      "üîπ CharacterTextSplitter\n",
      "- Splits strictly by character count\n",
      "- Very predictable but may break sentences\n",
      "- No semantic awareness\n",
      "- Useful for uniform chunk sizes or simple text processing\n",
      "\n",
      "üîπ TokenTextSplitter\n",
      "- Splits based on tokens instead of characters\n",
      "- Aligns with LLM token limits\n",
      "- Reduces embedding inconsistency\n",
      "- Best for OpenAI models and embedding pipelines\n",
      "\n",
      "üîπ Recursive vs Character (Quick Difference)\n",
      "- Recursive = Meaning-aware + flexible splitting\n",
      "- Character = Fixed size chunks, no context awareness\n",
      "\n",
      "üîπ When to Use What?\n",
      "- Use RecursiveCharacterTextSplitter ‚Üí Real world RAG systems\n",
      "- Use TokenTextSplitter ‚Üí LLM / embedding token-safe chunks\n",
      "- Use CharacterTextSplitter ‚Üí Simple or controlled experiments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìå Comparison of Text Splitting Methods\\n\")\n",
    "\n",
    "print(\"üîπ RecursiveCharacterTextSplitter\")\n",
    "print(\"- Smart splitting using hierarchical rules (paragraph ‚Üí sentence ‚Üí word ‚Üí char)\")\n",
    "print(\"- Preserves semantic meaning better\")\n",
    "print(\"- Adds overlap only when chunks are actually split\")\n",
    "print(\"- Best for RAG, documents, PDFs, articles\\n\")\n",
    "\n",
    "print(\"üîπ CharacterTextSplitter\")\n",
    "print(\"- Splits strictly by character count\")\n",
    "print(\"- Very predictable but may break sentences\")\n",
    "print(\"- No semantic awareness\")\n",
    "print(\"- Useful for uniform chunk sizes or simple text processing\\n\")\n",
    "\n",
    "print(\"üîπ TokenTextSplitter\")\n",
    "print(\"- Splits based on tokens instead of characters\")\n",
    "print(\"- Aligns with LLM token limits\")\n",
    "print(\"- Reduces embedding inconsistency\")\n",
    "print(\"- Best for OpenAI models and embedding pipelines\\n\")\n",
    "\n",
    "print(\"üîπ Recursive vs Character (Quick Difference)\")\n",
    "print(\"- Recursive = Meaning-aware + flexible splitting\")\n",
    "print(\"- Character = Fixed size chunks, no context awareness\\n\")\n",
    "\n",
    "print(\"üîπ When to Use What?\")\n",
    "print(\"- Use RecursiveCharacterTextSplitter ‚Üí Real world RAG systems\")\n",
    "print(\"- Use TokenTextSplitter ‚Üí LLM / embedding token-safe chunks\")\n",
    "print(\"- Use CharacterTextSplitter ‚Üí Simple or controlled experiments\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f06a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0120d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExploringRAGs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
